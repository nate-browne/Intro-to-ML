{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with TensorFlow\n",
    "\n",
    "*TensorFlow* is a powerful open-source software library for numerical computation, particularly well suited and fine-tuned for large-scale machine learning. Its basic principle is simple; first define in Python a graph of computations to perform, and then TensorFlow takes the graph and runs it efficiently using optimized C++.\n",
    "\n",
    "Most importantly, it's possible to break the graph into several chunks and run them in parallel across multiple CPUs/GPUs. TensorFlow also supports distributed computing, so you can train colossal neural networks on humongous training sets in a reasonable amount of time by splitting the computations across hundreds of servers (see chapter 12). TensorFlow can train a network with millions of parameters on a training set composed of billions of instances with millions of features each. This shouldn't be a surprise, since TensorFlow was developed by Google's Brain team and powers many things like Google Cloud Speech, Google Photos, and Google Search\n",
    "\n",
    "Here's a table of open source Deep Learning libraries available (not exhaustive):\n",
    "\n",
    "Library | API | Platforms | Started by | Year\n",
    "--- | --- | --- | --- | ---\n",
    "Caffe | Python, C++, Matlab | Linux, macOS, Windows | y, Jia, UC Berkeley | 2013\n",
    "Deeplearning4j | Java, Scala, Clojure | Linux, macOS, Windows, Android | A. Gibson, J. Patterson | 2014\n",
    "H20 | Python, R | Linux, macOS, Windows | H20.ai | 2014\n",
    "MXNet | Python, C++, others | Linux, macOS, Windows, iOS, Android | DMLC | 2015\n",
    "TensorFlow | Python, C++ | Linux, macOS, Windows, iOS, Android | Google | 2015\n",
    "Theano | Python | Linux, macOS, iOS | University of Montreal | 2010\n",
    "Torch | C++, Lua | Linux, macOS, iOS, Android | R. Collobert, K. Kavukcuoglu, C. Farabet | 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Graph and Running It in a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, you can create an interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow program is typically split into two parts: the first part builds a computation graph (this is called the *construction phase*), and the second part runs it (this is called the *execution phase*). The construction phase typically builds a computation graph represeting the ML model and the computations required to train it. The execution phase generally runs a loop that evaluates a training step repeatedly (for example, one step per mmini-batch), gradually improving the model parameters. WE well go through an example shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Graphs\n",
    "\n",
    "Any node you create is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine in most cases, but sometimes you may want to manage multiple independent graphs. This can be done by creating a new `Graph` and temporariliy making it the default graph inside of a `with` block, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: in Jupyter (and in a Python shell) it's common to run the same commands repeatedly when experimenting. As a result, you may end up with a default graph containing multiple duplicate nodes. One solution is to restart the Jupyter kernel/Python shell, but a more convenient solution is to just reset the default graph by running `tf.reset_default_graph()`*\n",
    "\n",
    "## Lifecycle of a Node Value\n",
    "\n",
    "When you evaluate a node, TensorFlow automatically determines the set of nodes that it depends on and it evalutes those nodes first. For example, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this code defines a simple graph. Then it starts a session and runs the graph to evaluate `y`; TensorFlow will detect that `y` depends on `x` and that `x` depends on `w`, so it first will evaluate `w`, then `x`, then `y`, then returns the value of `y`. Finally, it will execute `z`. Once again, it detects that it needs `w` and `x`, but it *__will not__* reuse the result of the previous evaluation. In short, the above code evalutes the values of `x` and `w` twice.\n",
    "\n",
    "All node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs. A variable starts its life when its initializer (constructor) is run, and ends when the session is closed.\n",
    "\n",
    "If you want to evalute `y` and `z` efficiently without evaluating `w` and `x` twice (like the previous code) you must ask Tensorflow to evaluate both `y` and `z` in just one graph run, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: in single-process TensorFlow, multiple sessions don't share any state, even if they reuse the same graph. In distributed TensorFlow (see chapter 12), variable state is stored on the servers, not in the sessions, so multiple sessions can share the same variables.\n",
    "\n",
    "## Linear Regression with TensorFlow\n",
    "\n",
    "TensorFlow operations (known as *ops* for short), can take any number of inputs and produce any number of outputs. For example, addition and multiplication both take two inputs and produce one output. Constants and variables are known as *source ops* (ops that take no input). The inputs and outputs are multidimensional arrays known as *tensors*. Just like Numpy arrays, tensors have a type and shape (in fact, the Python API tensors are just Numpy ndarrays. They typically contain floats, but you can use them to carry strings as well (arbitrary byte arrays)).\n",
    "\n",
    "In our examples, the tensors have just contained a single scalar value, but you can (obviously) perform computations on arrays of any shape.\n",
    "\n",
    "The following code performs Linear Regression on the California housing data from earlier by manipulating 2D arrays. It starts by fetching the data, then adding an extra bias input feature to all training instances using Numpy, then it creates two TensorFlow constant nodes `X` and `y`, to hold this data and the targets, and it uses some matrix operations provided by TensorFlow to define `theta`. You may recognize that `theta` is the Normal Equation ($\\hat{\\theta} = (\\textbf{X}^T\\cdot\\textbf{X})^{-1}\\cdot\\textbf{X}^T\\cdot y$; see chapter 4) Finally, the code creates a session and evaluates `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from os.path import expanduser\n",
    "\n",
    "housing = fetch_california_housing(data_home=expanduser('~/Coding Stuff/Python/handson-ml/datasets'))\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.69419202e+01],\n",
       "       [ 4.36693293e-01],\n",
       "       [ 9.43577803e-03],\n",
       "       [-1.07322041e-01],\n",
       "       [ 6.45065694e-01],\n",
       "       [-3.97638942e-06],\n",
       "       [-3.78654266e-03],\n",
       "       [-4.21314378e-01],\n",
       "       [-4.34513755e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy version of what we just did\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "theta_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# scikit learn version\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of this code vs using Numpy to compute the Normal Equation is that TensorFlow will use your GPU if you have one (providing you installed TensorFlow with GPU support (see chapter 12)).\n",
    "\n",
    "## Implementing Gradient Descent\n",
    "\n",
    "Now it's time to use Batch Gradient Descent. to do this, we manually compute the gradients, then we will use TensorFlow's autodiff feature to let TensorFlow compute the gradients automatically, and finally we'll use a few of TensorFlow's ootb optimizers.\n",
    "\n",
    "*Note: when using Gradient Descent, remember that it's important to first normailze the input feature vectors, or else training may be much slower. You can do this with TensorFlow, Numpy, sklearn's `StandardScaler`, or any other solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Compute the Gradients\n",
    "\n",
    "The next code is pretty easy to understand except for a few new elements:\n",
    "\n",
    "* The `random_uniform()` function creates a node in the graph that will generate a tensor containing random values, given its shape and value range, much like Numpys `rand()` function.\n",
    "\n",
    "* The `assign()` function creates a node that will assign a new value to a variable. In this case, it implements the Batch Gradient Descent step $\\theta^{(\\text{next step})} = \\theta - \\eta\\nabla_{\\theta}\\text{MSE}(\\theta)$\n",
    "\n",
    "* The main loop executes the training step over and over again (`n_epochs` times) and every 100 iteration it prints out the current Mean Squared Error (`mse`). It should be going down at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.52434105\n",
      "Epoch 500 MSE = 0.52432406\n",
      "Epoch 600 MSE = 0.52432144\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.5243211\n",
      "Epoch 900 MSE = 0.52432084\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff\n",
    "\n",
    "The preceding code works just fine, but it requires mathematically deriving the gradients from the cost function. This can get really, *really* messy if you use deep neural nets (or even SVMs, for that matter). You could use *symbolic differentiation* to automatically find the equations for the partial derivatives for you, but the resulting code wouldn't necessarily be efficient.\n",
    "\n",
    "To see why, consider $f(x) = \\exp(\\exp(\\exp(x)))$. You can figure out the derivative pretty easily, but your code wouldn't be as efficient as it would have been. A better solution would be to use dynamic programming to write a function that first computes $\\exp(x)$, then $\\exp(\\exp(x))$, then $\\exp(\\exp(\\exp(x)))$ and returns all three. This gives you $f(x)$ directly and if you need the derivative you can just multiply all three terms and you're done. The naïve approach would call the `exp` function nine times to compute both $f(x)$ and $f'(x)$. This approach would just call it 3 times. It gets worse if the function is defined by arbitrary code.\n",
    "\n",
    "Luckily, TensorFlow's autodiff features comes to the rescue; it can automatically and efficiently compute the gradients for you. Simply replace the `gradients = ...` line in the Gradient Descent code in the previous section with the following and the code will just work:\n",
    "`gradients = tf.gradients(mse, [theta])[0]`\n",
    "\n",
    "The `gradients()` function takes an op (in this case `mse` and a list of variables (in this case just `theta`) and it creates a list of ops (one per variable) to compute the gradients of the op with regards to each variable. So the `gradients` node will compute the gradient vectore of the MSE with regards to `theta`.\n",
    "\n",
    "There are four main approaches to computing gradients automatically. They're summarized in the following table. TensorFlow uses *reverse-mode autodiff*, which is perfect (efficient and accurate) when there are many inputs and few outputs, as is often the case in neural networks. It computes all of the partial derivatives of the outputs with regards to all of the inputs in just $n_{\\text{outputs}} 1$ graph traversals.\n",
    "\n",
    "Technique | # of graph traversals to compute all gradients | accuracy | supports arbitrary code | comment\n",
    "--- | --- | --- | --- | ---\n",
    "Numerical differentiation | $n_{\\text{inputs}} + 1$ | Low | yes | Trivial to implement\n",
    "Symbolic differentiation | N/A | High | no | Builds a very different graph\n",
    "Forward-mode autodiff | $n_{\\text{inputs}}$ | High | yes | uses *dual numbers*\n",
    "Reverse-mode autodiff | $n_{\\text{outputs}} + 1$ | high | yes | implemented by TensorFlow\n",
    "\n",
    "The following code implements the same as above but with autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.524341\n",
      "Epoch 500 MSE = 0.524324\n",
      "Epoch 600 MSE = 0.5243215\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.524321\n",
      "Epoch 900 MSE = 0.52432084\n",
      "Best theta: \n",
      "[[ 2.0685577 ]\n",
      " [ 0.8296404 ]\n",
      " [ 0.11875559]\n",
      " [-0.26556668]\n",
      " [ 0.30572915]\n",
      " [-0.00450187]\n",
      " [-0.03932704]\n",
      " [-0.8998372 ]\n",
      " [-0.87049496]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta: \\n{}\".format(best_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Optimizer\n",
    "\n",
    "So TensorFlow computes the gradients for you. But it gets even easier: it also provides a number of opimizers out of the box, including a `GradientDescentOptimizer`. You can simply replace the preceding `gradients = ...` and `training_op = ...` lines with the following, and everything will once again work out just fine:\n",
    "\n",
    "```\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```\n",
    "\n",
    "if you want to change the optimizer, you just have to change the first line. You could use a Momentum Optimizer (see chapter 11) by defining the optimizer like: `optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)`\n",
    "\n",
    "The code is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.maoptmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.524341\n",
      "Epoch 500 MSE = 0.524324\n",
      "Epoch 600 MSE = 0.5243215\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.524321\n",
      "Epoch 900 MSE = 0.52432084\n",
      "Best theta:\n",
      "[[ 2.0685577 ]\n",
      " [ 0.8296404 ]\n",
      " [ 0.11875559]\n",
      " [-0.26556668]\n",
      " [ 0.30572915]\n",
      " [-0.00450187]\n",
      " [-0.03932704]\n",
      " [-0.8998372 ]\n",
      " [-0.87049496]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use a momentum optimizer\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5244434\n",
      "Epoch 200 MSE = 0.5243208\n",
      "Epoch 300 MSE = 0.52432084\n",
      "Epoch 400 MSE = 0.52432096\n",
      "Epoch 500 MSE = 0.52432096\n",
      "Epoch 600 MSE = 0.52432096\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.52432096\n",
      "Epoch 900 MSE = 0.52432096\n",
      "Best theta:\n",
      "[[ 2.0685582 ]\n",
      " [ 0.82961947]\n",
      " [ 0.11875169]\n",
      " [-0.26552713]\n",
      " [ 0.30569643]\n",
      " [-0.00450299]\n",
      " [-0.03932627]\n",
      " [-0.8998851 ]\n",
      " [-0.8705405 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm\n",
    "\n",
    "Let's try to modify the previous code to implement Mini-batch Gradient Descent. For this, we need a way to replace `X` and `y` at every iteration with the next mini-batch. The simplest way to do this is to use placeholder nodes. These nodes are special because they don't actually perform any computation, they just output the data you tell them to at runtime. They're typically used to pass training data to TensorFlow during training. If you don't specify a value at runtime for a placeholder, you get an exception.\n",
    "\n",
    "To create one, you must call the `placeholder()` function and specify the output tensor's datatype. Optionally, you can also specify its shape if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a `Saver` saves and resores all variables under their own name, but if you need more control, you can specify which variables to save or restore and what names to use. For example, the following `Saver` will save or restore only the `theta` variable under the name `weights`:\n",
    "`saver = tf.train.Saver({'weights': theta})`\n",
    "\n",
    "By default, the `save()` method also saves the structure of the graph in a second file with the same name plus a `.meta` extension. You can load this graph structure using the `tf.train.import_meta_graph()`. This adds the graph to the default graph, and returns a `Saver` instance that you can then use to restore the graph's state (i.e., the variable values):\n",
    "\n",
    "```\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    [...]\n",
    "```\n",
    "\n",
    "This allows you to fully restore a saved model including both the graph structure and the variable values, without having to search for the code that built it. Next up though, mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
