{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Artificial Neural Networks\n",
    "\n",
    "*Artificial Neural Networks* (ANNs) are inspired by the architecture of the brain. ANNs are at the core of Deep Learning. They're versatile, powerful, and scalable, making them ideal to tackle large and highly complex ML tasks such as classifying billions of images (e.g. Google Images), powering speech recognition services (e.g. Siri, Cortana, etc), recommending the best videso to watch to hundreds of millions of users daily (e.g. the YouTube algorithm), or learning to beat the world champion at the game of *Go* by examining millions of paast games and then playing against istelf (DeepMind's AlphaGo).\n",
    "\n",
    "This chapter will introduce ANNs, starting with a quick tour of the first ANN architectures. We'll then present *Multi-Layer Perceptrons* (MLPs) and implement one using TensorFlow to tackle the MNIST digit classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Biological to Artificial Neurons\n",
    "\n",
    "ANNs are surprisingly old; they were first introduced in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts in their landmark paper [\"A Logical Calculus of Ideas Immanent in Nervous Activity\"](https://goo.gl/Ul4mxW). They presented a simplified computational model of how biological neurons may work together in animal brains to perform complex computations using *propositional logic*. This was the first ANN architecture, and since then many others have been invented.\n",
    "\n",
    "The early successes of ANNS till the 60s led to the belief that truly intelligent machines would exist. The funding went elsewhere when people realized that that dream wouldn't be feasible, but in the early 80s there was a revival of interest in ANNs as the new network architectures were invented and better training techniques were developed. By the 90s, powerful alternative ML techniques such as SVMs were favored by most researchers as they seemed to offer better results and stronger theoretical foundations.\n",
    "\n",
    "We're in another ANN renaissance, but this one may last because:\n",
    "\n",
    "* There's a __huge__ quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "\n",
    "* The tremendous increase in computing power since the 90s now makes it possible to train large neural nets in a reasonable amount of time. This is partially due to Moore's Law, but also due to powerful GPUs being developed by the gaming industry.\n",
    "\n",
    "* The training algos have improved. They're really only slightly different than the ones from the 90s, but those tweaks have a huge positive impact.\n",
    "\n",
    "* Some theoretical limits of ANNs have turned out to be benign in practice. (One example is how people thought that training algos would get stuck at local optima but that's rather rare in practice).\n",
    "\n",
    "* ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more attention and funding towards them, resulting in more progress and products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "\n",
    "*Perceptrons* are one of the simplest ANN architectures. They were invented in 1957 by Frank Rosenblatt (yup, that guy). It's blased on an artificial neuron known as a *Linear Threshold Unit* (LTU); the inputs and outputs are numbers and each input connection is associated with a weight. The LTU computes a weighted sum of the inputs ($z = w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\textbf{w}^T \\cdot \\textbf{x}$), then applies a *step function* to that sum and outputs the result: $h_w(\\textbf{x}) = \\text{step}(z) = \\text{step}(\\textbf{w}^T \\cdot \\textbf{x})$\n",
    "\n",
    "The most common step function used in Perceptrons is the *Heaviside step function* given in the next equation:\n",
    "\n",
    "$$\\text{heaviside }(z) = \\left\\{\\begin{array}{ll} 0 &\\text{ if } z \\lt 0 \\\\ 1 &\\text{ if } z \\geq 0\\end{array}\\right.$$\n",
    "\n",
    "Another common function is the *sign function* given below:\n",
    "\n",
    "$$\\text{sgn }(z) = \\left\\{\\begin{array}{ll} -1 &\\text{ if } z \\lt 0 \\\\ 0 &\\text{ if } z = 0 \\\\ 1 &\\text{ if } z \\gt 0\\end{array}\\right.$$\n",
    "\n",
    "A single LTU can be used for simple linear binary calssification. It computes a linear combination of the inputs and if the result exceeds a threshold, it outputs the positive class (else outputs the negative class). This works like a Logistic Regression classifier or a Linear SVM. You could use a single LTU to classify iris flowers based on petal length and width (with an extra bias feature $x_0 = 1$ like in the previous chapters). Training an LTU means finding the right values for $w_0, w_1, \\text{ and } w_2$.\n",
    "\n",
    "A Perceptron is a single layer of LTUs with each neuron connected to all of the inputs. These connections are often represented using special pass-through neurons called *input neurons*; they just output whatever input they're fed. Moreover, an extra bias feature is generally added ($x_0 = 1$). This bias feature is typically represented using a special type of neuron called a *bias neuron*, which just outputs 1 all of the time.\n",
    "\n",
    "The first training algo for Perceptrons (proposed by Rosenblatt) was largely inspired by *Hebb's rule*. In his book *The Organization of Behavior*, Hebb suggested that when a biologial neuron triggers another neuron, the connection between these two neurons grows stronger. This rule became known as *Hebbian learning*; that is, the connection weight between two neurons is increased whenever they have the same output. Perceptrons are trained using a variant of this rule that accounts for the error made by the network (it doesn't reinforce connections that lead to the wrong output). The equation is given below:\n",
    "\n",
    "$$w_{i, j}^{(\\text{next step})} = w_{i, j} + \\eta\\Big(y_j - \\hat{y}_j\\Big)x_i$$\n",
    "\n",
    "* $w_{i, j}$ is the connection weight between the i<sup>th</sup> input neuron and the j<sup>th</sup> output nuron.\n",
    "* $x_i$ is the i<sup>th</sup> input value of the current training instance.\n",
    "* $\\hat{y}_j$ is the output of the j<sup>th</sup> output neuron for the current training instance.\n",
    "* $y_i$ is the target output of the j<sup>th</sup> output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate.\n",
    "\n",
    "The decision boundary of each output neron is linear, so Perceptrons are incapable of learning complex patters (just like Logistic Regression classifiers). However, if the traiing instanes are linearly separable, Rosenblatt demonstrated that this algo will converge to a solution. This is called the *Perceptron convergence theorem*.\n",
    "\n",
    "Scikit-Learn provides a `Perceptron` class that implements a single LTU network. It can be used exactly as expected (here using the iris dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris-Setosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42, max_iter=100)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons strongly resemble Stochastic Gradient Descent, and sklearn's `Perceptron` class is equivalent to the `SGDClassifier` class with the following hyperparams: `loss='perceptron'`, `learning_rate='constant'`, `eta0=1` (learning rate), and `penalty=None` (no regularization).\n",
    "\n",
    "Contrary to logistic regression classifiers, Perceptrons don't output a class probability; rather, they make predictions based on a hard threshold. This is one of the good reasons to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "Many of the limits of Perceptrons (like that they can't solve some trivial problems) are eliminated by stacking multiple Perceptrons. The resulting ANN is known as a *Multi-Layer Perceptron* (MLP). An MLP is capable of solving the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of one (passthrough) input layer, one or more layers of LTUs called *hidden layers*, and one final layer of LTUs called the *output layer*. Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called a *deep neural network* (DNN).\n",
    "\n",
    "Researchers struggled to find a way to train MLPs without success for years, but in 1986, D.E. Rumelhart et al. published a [groundbreaking article](https://goo.gl/Wl7Xyc) introducing *backpropagation* (we know it today as Gradient Descent using reverse-mode autodiff; Gradient Descent was introduced in chapter 4 and autodiff was introduced in chapter 9)\n",
    "\n",
    "For each training instance, the algo feeds it to the network and computes the output of each neuron in each consecutive layer (this is the forward pass). It then measures the network's output error (i.e. the difference between the desired output and the actual output of the network) and it computes how much each neuron in the last hidden layer contributed to each output neuron's error. It then proceeds to measure how much of these error contributions came from each neuron in the previous hidden layer–and so on until the algo reaches the input layer. This revese pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (hence the name).\n",
    "\n",
    "If you check out the reverse-mode autodiff algo in the book in Appendix D, you'll find that the forward and reverse passses of backprop simply perform this autodiff. The last step of backprop is a Gradient Descent step on all the connection weights in the network using the error gradients measured earlier.\n",
    "\n",
    "In order for this algo to work, the authors made a key change to the MLP architecture: they replaced the step function with the logistic function $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ This was essential because the step function contains only flat segments (so there isn't a gradient) while the logistic function has a well-defined, nonzero derivative everywhere. The backprop algo can be used with other *activiation functions* instead of the logistic function. Two other popular ones include:\n",
    "\n",
    "* *The hyperbolic tangent function $tanh(z) = 2\\sigma(2z) - 1$*\n",
    "\n",
    "Just like the logistic function, it's S-shaped, continuous, and differentiable, but its output value ranges from -1 to 1 (instead of 0 to 1 like in logistic function) which tends to make each layer's output more or less normalized (i.e. centered around 0) at the beginning of training. This often helps speed up convergence.\n",
    "   \n",
    "* *The ReLU function (introduced in Chapter 9)*\n",
    "\n",
    "$\\text{ReLU}(z) = \\text{ max }(0, z)$ is continuous but unfortunately not differentiable at $z = 0$ (the slope changes abruptly, which can make Gradient Descent bounce around). It works very well in practice and has the advantage of being fast to compute. Most importatly, the fact that it doesn't have a maximum output value also helps reduce some issues during Gradient Descent (we'll revisit this in the next chapter)\n",
    "\n",
    "An MLP is often used for classification, with each output corresponding to a different binary class. When the classes are exclusive (like digits 0-9), the output layer is typically modified by replacing the individual activiation functions by a shared *softmax* function. The softmax function was introduced back in Chapter 4. The output of each neuron then corresponds to the estimated probability of the corresponding class. Signal only flows one-way, making this architecture an example of a *feedforward neural network* (FNN).\n",
    "\n",
    "*Note: biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck to sigmoid functions for a very long time. Turns out that the ReLU activation function generally works better in ANNs though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MLP with TensorFlow's High-Level API\n",
    "\n",
    "The simplest way to train an MLP with TensorFlow is to use the high-level API tf.learn, which offers a sklearn-compatible API. The `DNNClassifier` class makes it fairly easy to train a deep neural network with any number of hidden layers and a softmax output layer to output estimated class probabilities. For example, the following code trains a DNN for classification with two hidden layers (one with 300 neurons, one with 100) and a softmax output layer with 10 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, separate the data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x107d22a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck/model.ckpt.\n",
      "INFO:tensorflow:loss = 116.91783, step = 1\n",
      "INFO:tensorflow:global_step/sec: 351.232\n",
      "INFO:tensorflow:loss = 19.726152, step = 101 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.847\n",
      "INFO:tensorflow:loss = 7.7188935, step = 201 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.738\n",
      "INFO:tensorflow:loss = 9.86439, step = 301 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.409\n",
      "INFO:tensorflow:loss = 5.4685082, step = 401 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.829\n",
      "INFO:tensorflow:loss = 3.7951076, step = 501 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.148\n",
      "INFO:tensorflow:loss = 3.3858142, step = 601 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.145\n",
      "INFO:tensorflow:loss = 5.9261193, step = 701 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.598\n",
      "INFO:tensorflow:loss = 8.071234, step = 801 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.159\n",
      "INFO:tensorflow:loss = 4.448347, step = 901 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.457\n",
      "INFO:tensorflow:loss = 2.072418, step = 1001 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.625\n",
      "INFO:tensorflow:loss = 13.688567, step = 1101 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.137\n",
      "INFO:tensorflow:loss = 0.8989541, step = 1201 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.589\n",
      "INFO:tensorflow:loss = 5.770926, step = 1301 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.925\n",
      "INFO:tensorflow:loss = 2.3390534, step = 1401 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.369\n",
      "INFO:tensorflow:loss = 4.9973826, step = 1501 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.153\n",
      "INFO:tensorflow:loss = 2.9227006, step = 1601 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.88\n",
      "INFO:tensorflow:loss = 1.0663614, step = 1701 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 436.443\n",
      "INFO:tensorflow:loss = 4.570287, step = 1801 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.759\n",
      "INFO:tensorflow:loss = 5.8018336, step = 1901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.947\n",
      "INFO:tensorflow:loss = 6.650654, step = 2001 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.549\n",
      "INFO:tensorflow:loss = 2.8465805, step = 2101 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.957\n",
      "INFO:tensorflow:loss = 4.1538043, step = 2201 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.934\n",
      "INFO:tensorflow:loss = 1.5273781, step = 2301 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.202\n",
      "INFO:tensorflow:loss = 2.3837633, step = 2401 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.689\n",
      "INFO:tensorflow:loss = 0.21190888, step = 2501 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 432.39\n",
      "INFO:tensorflow:loss = 1.6201332, step = 2601 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.79\n",
      "INFO:tensorflow:loss = 4.737374, step = 2701 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.334\n",
      "INFO:tensorflow:loss = 1.1893677, step = 2801 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.913\n",
      "INFO:tensorflow:loss = 5.800568, step = 2901 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.532\n",
      "INFO:tensorflow:loss = 0.9686631, step = 3001 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.498\n",
      "INFO:tensorflow:loss = 4.457382, step = 3101 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.244\n",
      "INFO:tensorflow:loss = 0.6395981, step = 3201 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.344\n",
      "INFO:tensorflow:loss = 0.67422074, step = 3301 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.983\n",
      "INFO:tensorflow:loss = 1.3755867, step = 3401 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.908\n",
      "INFO:tensorflow:loss = 1.0384066, step = 3501 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.717\n",
      "INFO:tensorflow:loss = 5.177674, step = 3601 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.399\n",
      "INFO:tensorflow:loss = 2.907247, step = 3701 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.155\n",
      "INFO:tensorflow:loss = 2.8655643, step = 3801 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.635\n",
      "INFO:tensorflow:loss = 4.22704, step = 3901 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.35\n",
      "INFO:tensorflow:loss = 3.304573, step = 4001 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.054\n",
      "INFO:tensorflow:loss = 0.85491717, step = 4101 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.94\n",
      "INFO:tensorflow:loss = 0.91825724, step = 4201 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.5\n",
      "INFO:tensorflow:loss = 3.3899827, step = 4301 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.469\n",
      "INFO:tensorflow:loss = 1.7377034, step = 4401 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.784\n",
      "INFO:tensorflow:loss = 0.9046244, step = 4501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.575\n",
      "INFO:tensorflow:loss = 0.30265853, step = 4601 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.247\n",
      "INFO:tensorflow:loss = 1.262469, step = 4701 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 452.945\n",
      "INFO:tensorflow:loss = 0.7259762, step = 4801 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.766\n",
      "INFO:tensorflow:loss = 5.150702, step = 4901 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.067\n",
      "INFO:tensorflow:loss = 3.390099, step = 5001 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.301\n",
      "INFO:tensorflow:loss = 0.58473134, step = 5101 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.19\n",
      "INFO:tensorflow:loss = 4.6683683, step = 5201 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.507\n",
      "INFO:tensorflow:loss = 6.7524157, step = 5301 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.457\n",
      "INFO:tensorflow:loss = 1.3630488, step = 5401 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.622\n",
      "INFO:tensorflow:loss = 0.6976003, step = 5501 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.918\n",
      "INFO:tensorflow:loss = 1.0238447, step = 5601 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.239\n",
      "INFO:tensorflow:loss = 1.3833302, step = 5701 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.068\n",
      "INFO:tensorflow:loss = 0.16660154, step = 5801 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.758\n",
      "INFO:tensorflow:loss = 0.94982773, step = 5901 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.265\n",
      "INFO:tensorflow:loss = 0.42684162, step = 6001 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.616\n",
      "INFO:tensorflow:loss = 0.25904816, step = 6101 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.347\n",
      "INFO:tensorflow:loss = 2.835022, step = 6201 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.43\n",
      "INFO:tensorflow:loss = 2.3998013, step = 6301 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.51\n",
      "INFO:tensorflow:loss = 1.2665954, step = 6401 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.149\n",
      "INFO:tensorflow:loss = 0.854808, step = 6501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.947\n",
      "INFO:tensorflow:loss = 0.21280341, step = 6601 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.358\n",
      "INFO:tensorflow:loss = 0.24261421, step = 6701 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.074\n",
      "INFO:tensorflow:loss = 0.6359216, step = 6801 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 476.438\n",
      "INFO:tensorflow:loss = 0.76477313, step = 6901 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.446\n",
      "INFO:tensorflow:loss = 0.41813433, step = 7001 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.838\n",
      "INFO:tensorflow:loss = 1.672606, step = 7101 (0.227 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 483.199\n",
      "INFO:tensorflow:loss = 0.30129975, step = 7201 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.739\n",
      "INFO:tensorflow:loss = 0.16883413, step = 7301 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.15\n",
      "INFO:tensorflow:loss = 1.0265357, step = 7401 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.923\n",
      "INFO:tensorflow:loss = 0.6697571, step = 7501 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.799\n",
      "INFO:tensorflow:loss = 0.05456145, step = 7601 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.235\n",
      "INFO:tensorflow:loss = 0.30643904, step = 7701 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.284\n",
      "INFO:tensorflow:loss = 0.14411016, step = 7801 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.941\n",
      "INFO:tensorflow:loss = 1.7541127, step = 7901 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.814\n",
      "INFO:tensorflow:loss = 0.24501503, step = 8001 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.829\n",
      "INFO:tensorflow:loss = 0.26312822, step = 8101 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.303\n",
      "INFO:tensorflow:loss = 0.843489, step = 8201 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.769\n",
      "INFO:tensorflow:loss = 0.5882916, step = 8301 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.184\n",
      "INFO:tensorflow:loss = 0.07562019, step = 8401 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.973\n",
      "INFO:tensorflow:loss = 0.6229721, step = 8501 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.295\n",
      "INFO:tensorflow:loss = 0.18821973, step = 8601 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 404.088\n",
      "INFO:tensorflow:loss = 0.23059833, step = 8701 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.124\n",
      "INFO:tensorflow:loss = 0.4294447, step = 8801 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.004\n",
      "INFO:tensorflow:loss = 0.042745166, step = 8901 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.277\n",
      "INFO:tensorflow:loss = 0.06778384, step = 9001 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.715\n",
      "INFO:tensorflow:loss = 0.039299257, step = 9101 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.856\n",
      "INFO:tensorflow:loss = 0.5841108, step = 9201 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.203\n",
      "INFO:tensorflow:loss = 0.48709983, step = 9301 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.934\n",
      "INFO:tensorflow:loss = 0.47624317, step = 9401 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.239\n",
      "INFO:tensorflow:loss = 0.016786162, step = 9501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.482\n",
      "INFO:tensorflow:loss = 1.5765346, step = 9601 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.403\n",
      "INFO:tensorflow:loss = 0.06947713, step = 9701 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.044\n",
      "INFO:tensorflow:loss = 0.23370942, step = 9801 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.093\n",
      "INFO:tensorflow:loss = 0.85318613, step = 9901 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.894\n",
      "INFO:tensorflow:loss = 0.23429659, step = 10001 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.035\n",
      "INFO:tensorflow:loss = 0.15588129, step = 10101 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.292\n",
      "INFO:tensorflow:loss = 0.06137342, step = 10201 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.089\n",
      "INFO:tensorflow:loss = 0.37595305, step = 10301 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 430.789\n",
      "INFO:tensorflow:loss = 0.19710428, step = 10401 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.748\n",
      "INFO:tensorflow:loss = 1.0033785, step = 10501 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.938\n",
      "INFO:tensorflow:loss = 0.13775413, step = 10601 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.883\n",
      "INFO:tensorflow:loss = 3.2915823, step = 10701 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.027\n",
      "INFO:tensorflow:loss = 0.07428906, step = 10801 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.685\n",
      "INFO:tensorflow:loss = 0.22757138, step = 10901 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.396\n",
      "INFO:tensorflow:loss = 0.48002052, step = 11001 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.06\n",
      "INFO:tensorflow:loss = 0.23839596, step = 11101 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.008\n",
      "INFO:tensorflow:loss = 0.16563441, step = 11201 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.411\n",
      "INFO:tensorflow:loss = 0.10971592, step = 11301 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 452.38\n",
      "INFO:tensorflow:loss = 0.09998305, step = 11401 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.896\n",
      "INFO:tensorflow:loss = 0.13215777, step = 11501 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.251\n",
      "INFO:tensorflow:loss = 0.0644943, step = 11601 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.868\n",
      "INFO:tensorflow:loss = 0.03856142, step = 11701 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.278\n",
      "INFO:tensorflow:loss = 0.100505054, step = 11801 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.157\n",
      "INFO:tensorflow:loss = 0.31176642, step = 11901 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.822\n",
      "INFO:tensorflow:loss = 0.20806266, step = 12001 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.078\n",
      "INFO:tensorflow:loss = 0.20825098, step = 12101 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.775\n",
      "INFO:tensorflow:loss = 0.117493525, step = 12201 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.18\n",
      "INFO:tensorflow:loss = 0.24327035, step = 12301 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.104\n",
      "INFO:tensorflow:loss = 0.029082797, step = 12401 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.108\n",
      "INFO:tensorflow:loss = 0.21549556, step = 12501 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.366\n",
      "INFO:tensorflow:loss = 0.10451818, step = 12601 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.937\n",
      "INFO:tensorflow:loss = 0.042015497, step = 12701 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.958\n",
      "INFO:tensorflow:loss = 0.07951188, step = 12801 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.509\n",
      "INFO:tensorflow:loss = 0.07616199, step = 12901 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.151\n",
      "INFO:tensorflow:loss = 0.048359513, step = 13001 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.939\n",
      "INFO:tensorflow:loss = 0.053253137, step = 13101 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.515\n",
      "INFO:tensorflow:loss = 0.09869914, step = 13201 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.226\n",
      "INFO:tensorflow:loss = 0.18696164, step = 13301 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.993\n",
      "INFO:tensorflow:loss = 0.19022202, step = 13401 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.403\n",
      "INFO:tensorflow:loss = 0.10446543, step = 13501 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.514\n",
      "INFO:tensorflow:loss = 0.008732419, step = 13601 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.527\n",
      "INFO:tensorflow:loss = 0.124580994, step = 13701 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.568\n",
      "INFO:tensorflow:loss = 0.8414895, step = 13801 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.717\n",
      "INFO:tensorflow:loss = 0.04182204, step = 13901 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 455.599\n",
      "INFO:tensorflow:loss = 0.14431645, step = 14001 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.34\n",
      "INFO:tensorflow:loss = 0.009873925, step = 14101 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.92\n",
      "INFO:tensorflow:loss = 0.30146375, step = 14201 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.409\n",
      "INFO:tensorflow:loss = 0.14114034, step = 14301 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.113\n",
      "INFO:tensorflow:loss = 0.019472148, step = 14401 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.983\n",
      "INFO:tensorflow:loss = 0.042265702, step = 14501 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.451\n",
      "INFO:tensorflow:loss = 0.004953034, step = 14601 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 455.396\n",
      "INFO:tensorflow:loss = 0.013284525, step = 14701 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 390.037\n",
      "INFO:tensorflow:loss = 0.072848, step = 14801 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.704\n",
      "INFO:tensorflow:loss = 0.11122689, step = 14901 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.314\n",
      "INFO:tensorflow:loss = 0.014131929, step = 15001 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.614\n",
      "INFO:tensorflow:loss = 0.06370776, step = 15101 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.053\n",
      "INFO:tensorflow:loss = 1.449235, step = 15201 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.711\n",
      "INFO:tensorflow:loss = 0.1452581, step = 15301 (0.266 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 476.54\n",
      "INFO:tensorflow:loss = 0.08832723, step = 15401 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.33\n",
      "INFO:tensorflow:loss = 0.07513741, step = 15501 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.469\n",
      "INFO:tensorflow:loss = 0.1700801, step = 15601 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.818\n",
      "INFO:tensorflow:loss = 0.003381093, step = 15701 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.055\n",
      "INFO:tensorflow:loss = 0.018435977, step = 15801 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.194\n",
      "INFO:tensorflow:loss = 0.025203666, step = 15901 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.195\n",
      "INFO:tensorflow:loss = 0.19151206, step = 16001 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.278\n",
      "INFO:tensorflow:loss = 0.13565953, step = 16101 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.127\n",
      "INFO:tensorflow:loss = 0.2928977, step = 16201 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.568\n",
      "INFO:tensorflow:loss = 0.2682612, step = 16301 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.584\n",
      "INFO:tensorflow:loss = 0.039399497, step = 16401 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.561\n",
      "INFO:tensorflow:loss = 0.03593642, step = 16501 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.496\n",
      "INFO:tensorflow:loss = 0.08533412, step = 16601 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.02\n",
      "INFO:tensorflow:loss = 0.15456226, step = 16701 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.109\n",
      "INFO:tensorflow:loss = 0.0100018205, step = 16801 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 476.994\n",
      "INFO:tensorflow:loss = 0.002236384, step = 16901 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.701\n",
      "INFO:tensorflow:loss = 0.029196542, step = 17001 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 353.886\n",
      "INFO:tensorflow:loss = 0.050280727, step = 17101 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.242\n",
      "INFO:tensorflow:loss = 0.020657735, step = 17201 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.915\n",
      "INFO:tensorflow:loss = 0.084781684, step = 17301 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.872\n",
      "INFO:tensorflow:loss = 0.020456158, step = 17401 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.052\n",
      "INFO:tensorflow:loss = 0.048276134, step = 17501 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.837\n",
      "INFO:tensorflow:loss = 0.041990586, step = 17601 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.637\n",
      "INFO:tensorflow:loss = 0.103546515, step = 17701 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.411\n",
      "INFO:tensorflow:loss = 0.10435408, step = 17801 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.219\n",
      "INFO:tensorflow:loss = 0.012530031, step = 17901 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.696\n",
      "INFO:tensorflow:loss = 0.0174459, step = 18001 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.288\n",
      "INFO:tensorflow:loss = 0.017381273, step = 18101 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.606\n",
      "INFO:tensorflow:loss = 0.02091244, step = 18201 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.809\n",
      "INFO:tensorflow:loss = 0.21941018, step = 18301 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.609\n",
      "INFO:tensorflow:loss = 0.02223645, step = 18401 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.812\n",
      "INFO:tensorflow:loss = 0.1824435, step = 18501 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.988\n",
      "INFO:tensorflow:loss = 0.09118988, step = 18601 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.187\n",
      "INFO:tensorflow:loss = 0.010198857, step = 18701 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.07\n",
      "INFO:tensorflow:loss = 0.024310194, step = 18801 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.665\n",
      "INFO:tensorflow:loss = 0.036324147, step = 18901 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.111\n",
      "INFO:tensorflow:loss = 0.0069587617, step = 19001 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.054\n",
      "INFO:tensorflow:loss = 0.055735387, step = 19101 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.921\n",
      "INFO:tensorflow:loss = 0.014559743, step = 19201 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.077\n",
      "INFO:tensorflow:loss = 0.01782643, step = 19301 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.947\n",
      "INFO:tensorflow:loss = 0.14601183, step = 19401 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.383\n",
      "INFO:tensorflow:loss = 0.34826216, step = 19501 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.395\n",
      "INFO:tensorflow:loss = 0.35205024, step = 19601 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.257\n",
      "INFO:tensorflow:loss = 0.117723316, step = 19701 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.158\n",
      "INFO:tensorflow:loss = 0.02545755, step = 19801 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.769\n",
      "INFO:tensorflow:loss = 0.023922402, step = 19901 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.208\n",
      "INFO:tensorflow:loss = 0.10384411, step = 20001 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.724\n",
      "INFO:tensorflow:loss = 0.08137699, step = 20101 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.205\n",
      "INFO:tensorflow:loss = 0.04808278, step = 20201 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.166\n",
      "INFO:tensorflow:loss = 0.034084555, step = 20301 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.054\n",
      "INFO:tensorflow:loss = 0.03736161, step = 20401 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.155\n",
      "INFO:tensorflow:loss = 0.06073036, step = 20501 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.53\n",
      "INFO:tensorflow:loss = 0.024951933, step = 20601 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.925\n",
      "INFO:tensorflow:loss = 0.032205872, step = 20701 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.613\n",
      "INFO:tensorflow:loss = 0.13331519, step = 20801 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.37\n",
      "INFO:tensorflow:loss = 0.06252038, step = 20901 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.943\n",
      "INFO:tensorflow:loss = 0.03257003, step = 21001 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.014\n",
      "INFO:tensorflow:loss = 0.025299385, step = 21101 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.518\n",
      "INFO:tensorflow:loss = 0.013352023, step = 21201 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.782\n",
      "INFO:tensorflow:loss = 0.025476119, step = 21301 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.509\n",
      "INFO:tensorflow:loss = 0.07202153, step = 21401 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.276\n",
      "INFO:tensorflow:loss = 0.04157439, step = 21501 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.237\n",
      "INFO:tensorflow:loss = 0.015274646, step = 21601 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.093\n",
      "INFO:tensorflow:loss = 0.093382284, step = 21701 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.056\n",
      "INFO:tensorflow:loss = 0.0185293, step = 21801 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.507\n",
      "INFO:tensorflow:loss = 0.084770225, step = 21901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.048\n",
      "INFO:tensorflow:loss = 0.02559017, step = 22001 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.586\n",
      "INFO:tensorflow:loss = 0.03442644, step = 22101 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.764\n",
      "INFO:tensorflow:loss = 0.038322493, step = 22201 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.66\n",
      "INFO:tensorflow:loss = 0.057616428, step = 22301 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.335\n",
      "INFO:tensorflow:loss = 0.012148071, step = 22401 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.24\n",
      "INFO:tensorflow:loss = 0.0019288493, step = 22501 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.985\n",
      "INFO:tensorflow:loss = 0.07399666, step = 22601 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.692\n",
      "INFO:tensorflow:loss = 0.057806585, step = 22701 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.138\n",
      "INFO:tensorflow:loss = 0.0077180164, step = 22801 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.449\n",
      "INFO:tensorflow:loss = 0.03202825, step = 22901 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.444\n",
      "INFO:tensorflow:loss = 0.01439732, step = 23001 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.295\n",
      "INFO:tensorflow:loss = 0.007116898, step = 23101 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.814\n",
      "INFO:tensorflow:loss = 0.06526931, step = 23201 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.968\n",
      "INFO:tensorflow:loss = 0.016332712, step = 23301 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.148\n",
      "INFO:tensorflow:loss = 0.04790019, step = 23401 (0.233 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 440.698\n",
      "INFO:tensorflow:loss = 0.027692752, step = 23501 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.223\n",
      "INFO:tensorflow:loss = 0.02722666, step = 23601 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.686\n",
      "INFO:tensorflow:loss = 0.020128762, step = 23701 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.608\n",
      "INFO:tensorflow:loss = 0.043561637, step = 23801 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.055\n",
      "INFO:tensorflow:loss = 0.044166967, step = 23901 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.88\n",
      "INFO:tensorflow:loss = 0.0037725873, step = 24001 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.032\n",
      "INFO:tensorflow:loss = 0.09617338, step = 24101 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.147\n",
      "INFO:tensorflow:loss = 0.066949494, step = 24201 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.067\n",
      "INFO:tensorflow:loss = 0.0013058052, step = 24301 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.385\n",
      "INFO:tensorflow:loss = 0.020831391, step = 24401 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.124\n",
      "INFO:tensorflow:loss = 0.014710723, step = 24501 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.843\n",
      "INFO:tensorflow:loss = 0.09680824, step = 24601 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.275\n",
      "INFO:tensorflow:loss = 0.06369573, step = 24701 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 465.334\n",
      "INFO:tensorflow:loss = 0.0053278157, step = 24801 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.207\n",
      "INFO:tensorflow:loss = 0.0025036288, step = 24901 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.563\n",
      "INFO:tensorflow:loss = 0.01646075, step = 25001 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.101\n",
      "INFO:tensorflow:loss = 0.12697473, step = 25101 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.393\n",
      "INFO:tensorflow:loss = 0.083347514, step = 25201 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.81\n",
      "INFO:tensorflow:loss = 0.038487315, step = 25301 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.013\n",
      "INFO:tensorflow:loss = 0.03706975, step = 25401 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.692\n",
      "INFO:tensorflow:loss = 0.07788524, step = 25501 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.693\n",
      "INFO:tensorflow:loss = 0.031319667, step = 25601 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.616\n",
      "INFO:tensorflow:loss = 0.005481447, step = 25701 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 436.989\n",
      "INFO:tensorflow:loss = 0.03248518, step = 25801 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.556\n",
      "INFO:tensorflow:loss = 0.047136776, step = 25901 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.527\n",
      "INFO:tensorflow:loss = 0.0852833, step = 26001 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.451\n",
      "INFO:tensorflow:loss = 0.009049022, step = 26101 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.891\n",
      "INFO:tensorflow:loss = 0.14385156, step = 26201 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.947\n",
      "INFO:tensorflow:loss = 0.036974195, step = 26301 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.67\n",
      "INFO:tensorflow:loss = 0.011686836, step = 26401 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.85\n",
      "INFO:tensorflow:loss = 0.042685937, step = 26501 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 444.502\n",
      "INFO:tensorflow:loss = 0.017331498, step = 26601 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 361.1\n",
      "INFO:tensorflow:loss = 0.066618375, step = 26701 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.328\n",
      "INFO:tensorflow:loss = 0.0060706744, step = 26801 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.168\n",
      "INFO:tensorflow:loss = 0.0060419496, step = 26901 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.007\n",
      "INFO:tensorflow:loss = 0.028329704, step = 27001 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.677\n",
      "INFO:tensorflow:loss = 0.022914365, step = 27101 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.84\n",
      "INFO:tensorflow:loss = 0.05674693, step = 27201 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.231\n",
      "INFO:tensorflow:loss = 0.01321584, step = 27301 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.919\n",
      "INFO:tensorflow:loss = 0.0022269478, step = 27401 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.574\n",
      "INFO:tensorflow:loss = 0.10788577, step = 27501 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.812\n",
      "INFO:tensorflow:loss = 0.0042846063, step = 27601 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.02\n",
      "INFO:tensorflow:loss = 0.025524836, step = 27701 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.94\n",
      "INFO:tensorflow:loss = 0.0017057407, step = 27801 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.071\n",
      "INFO:tensorflow:loss = 0.0013346367, step = 27901 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.342\n",
      "INFO:tensorflow:loss = 0.024104755, step = 28001 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.063\n",
      "INFO:tensorflow:loss = 0.006537707, step = 28101 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.532\n",
      "INFO:tensorflow:loss = 0.01485259, step = 28201 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.621\n",
      "INFO:tensorflow:loss = 0.064005, step = 28301 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.209\n",
      "INFO:tensorflow:loss = 0.02356694, step = 28401 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.365\n",
      "INFO:tensorflow:loss = 0.030663995, step = 28501 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.4\n",
      "INFO:tensorflow:loss = 0.016935008, step = 28601 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.44\n",
      "INFO:tensorflow:loss = 0.042927913, step = 28701 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.613\n",
      "INFO:tensorflow:loss = 0.0038882622, step = 28801 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.94\n",
      "INFO:tensorflow:loss = 0.0037480765, step = 28901 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.49\n",
      "INFO:tensorflow:loss = 0.06311649, step = 29001 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.79\n",
      "INFO:tensorflow:loss = 0.004021855, step = 29101 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.678\n",
      "INFO:tensorflow:loss = 0.016241753, step = 29201 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.746\n",
      "INFO:tensorflow:loss = 0.009790956, step = 29301 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.196\n",
      "INFO:tensorflow:loss = 0.04726413, step = 29401 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.006\n",
      "INFO:tensorflow:loss = 0.0035362758, step = 29501 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.952\n",
      "INFO:tensorflow:loss = 0.010727585, step = 29601 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.059\n",
      "INFO:tensorflow:loss = 0.004678381, step = 29701 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.677\n",
      "INFO:tensorflow:loss = 0.023802081, step = 29801 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.69\n",
      "INFO:tensorflow:loss = 0.024223682, step = 29901 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 432.591\n",
      "INFO:tensorflow:loss = 0.010199474, step = 30001 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.731\n",
      "INFO:tensorflow:loss = 0.018199269, step = 30101 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 476.102\n",
      "INFO:tensorflow:loss = 0.05224423, step = 30201 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.657\n",
      "INFO:tensorflow:loss = 0.010240788, step = 30301 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.924\n",
      "INFO:tensorflow:loss = 0.035873264, step = 30401 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.401\n",
      "INFO:tensorflow:loss = 0.003901624, step = 30501 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.134\n",
      "INFO:tensorflow:loss = 0.023709623, step = 30601 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.102\n",
      "INFO:tensorflow:loss = 0.06435669, step = 30701 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 414.663\n",
      "INFO:tensorflow:loss = 0.008748247, step = 30801 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.112\n",
      "INFO:tensorflow:loss = 0.03568624, step = 30901 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.994\n",
      "INFO:tensorflow:loss = 0.0153020015, step = 31001 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.024\n",
      "INFO:tensorflow:loss = 0.026052382, step = 31101 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.364\n",
      "INFO:tensorflow:loss = 0.0025365918, step = 31201 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.358\n",
      "INFO:tensorflow:loss = 0.023464367, step = 31301 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.227\n",
      "INFO:tensorflow:loss = 0.012320321, step = 31401 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 406.417\n",
      "INFO:tensorflow:loss = 0.0044600912, step = 31501 (0.246 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 426.854\n",
      "INFO:tensorflow:loss = 0.22976612, step = 31601 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.575\n",
      "INFO:tensorflow:loss = 0.01615125, step = 31701 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.657\n",
      "INFO:tensorflow:loss = 0.0356962, step = 31801 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.814\n",
      "INFO:tensorflow:loss = 0.00809639, step = 31901 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.936\n",
      "INFO:tensorflow:loss = 0.03560849, step = 32001 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.708\n",
      "INFO:tensorflow:loss = 0.013653827, step = 32101 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.449\n",
      "INFO:tensorflow:loss = 0.017421462, step = 32201 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.947\n",
      "INFO:tensorflow:loss = 0.051849775, step = 32301 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.142\n",
      "INFO:tensorflow:loss = 0.025272995, step = 32401 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.328\n",
      "INFO:tensorflow:loss = 0.090703145, step = 32501 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.874\n",
      "INFO:tensorflow:loss = 0.01760393, step = 32601 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.166\n",
      "INFO:tensorflow:loss = 0.021865195, step = 32701 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.611\n",
      "INFO:tensorflow:loss = 0.01439802, step = 32801 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.866\n",
      "INFO:tensorflow:loss = 0.034405872, step = 32901 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.432\n",
      "INFO:tensorflow:loss = 0.031457033, step = 33001 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.851\n",
      "INFO:tensorflow:loss = 0.038300805, step = 33101 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.315\n",
      "INFO:tensorflow:loss = 0.0014397462, step = 33201 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.887\n",
      "INFO:tensorflow:loss = 0.022177845, step = 33301 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.205\n",
      "INFO:tensorflow:loss = 0.009390414, step = 33401 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.497\n",
      "INFO:tensorflow:loss = 0.02805205, step = 33501 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 361.7\n",
      "INFO:tensorflow:loss = 0.044592924, step = 33601 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.977\n",
      "INFO:tensorflow:loss = 0.02649859, step = 33701 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.473\n",
      "INFO:tensorflow:loss = 0.05273034, step = 33801 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.845\n",
      "INFO:tensorflow:loss = 0.0015911504, step = 33901 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.382\n",
      "INFO:tensorflow:loss = 0.01412407, step = 34001 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.857\n",
      "INFO:tensorflow:loss = 0.016386997, step = 34101 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.44\n",
      "INFO:tensorflow:loss = 0.018718231, step = 34201 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.641\n",
      "INFO:tensorflow:loss = 0.035597384, step = 34301 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.819\n",
      "INFO:tensorflow:loss = 0.0023802402, step = 34401 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.257\n",
      "INFO:tensorflow:loss = 0.052753538, step = 34501 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.719\n",
      "INFO:tensorflow:loss = 0.015224776, step = 34601 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.382\n",
      "INFO:tensorflow:loss = 0.006820314, step = 34701 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.312\n",
      "INFO:tensorflow:loss = 0.004074874, step = 34801 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.041\n",
      "INFO:tensorflow:loss = 0.00558929, step = 34901 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.266\n",
      "INFO:tensorflow:loss = 0.000115512135, step = 35001 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.555\n",
      "INFO:tensorflow:loss = 0.013615127, step = 35101 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.184\n",
      "INFO:tensorflow:loss = 0.013087405, step = 35201 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.738\n",
      "INFO:tensorflow:loss = 0.015769698, step = 35301 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.313\n",
      "INFO:tensorflow:loss = 0.019565716, step = 35401 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.37\n",
      "INFO:tensorflow:loss = 0.0045301765, step = 35501 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.757\n",
      "INFO:tensorflow:loss = 0.020369481, step = 35601 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.906\n",
      "INFO:tensorflow:loss = 0.00224541, step = 35701 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.102\n",
      "INFO:tensorflow:loss = 0.013757249, step = 35801 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.839\n",
      "INFO:tensorflow:loss = 0.020183127, step = 35901 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.962\n",
      "INFO:tensorflow:loss = 0.008843105, step = 36001 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.816\n",
      "INFO:tensorflow:loss = 0.01668681, step = 36101 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.071\n",
      "INFO:tensorflow:loss = 0.04389746, step = 36201 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.47\n",
      "INFO:tensorflow:loss = 0.0023670858, step = 36301 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.743\n",
      "INFO:tensorflow:loss = 0.022333734, step = 36401 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.949\n",
      "INFO:tensorflow:loss = 0.014957, step = 36501 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.766\n",
      "INFO:tensorflow:loss = 0.041734107, step = 36601 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.988\n",
      "INFO:tensorflow:loss = 0.008133617, step = 36701 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 453.311\n",
      "INFO:tensorflow:loss = 0.0053994847, step = 36801 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 465.653\n",
      "INFO:tensorflow:loss = 0.056089446, step = 36901 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.847\n",
      "INFO:tensorflow:loss = 0.0031996681, step = 37001 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.999\n",
      "INFO:tensorflow:loss = 0.017151972, step = 37101 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.091\n",
      "INFO:tensorflow:loss = 0.038954847, step = 37201 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.041\n",
      "INFO:tensorflow:loss = 0.0077527445, step = 37301 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 430.975\n",
      "INFO:tensorflow:loss = 0.016257718, step = 37401 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.913\n",
      "INFO:tensorflow:loss = 0.0060559986, step = 37501 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.661\n",
      "INFO:tensorflow:loss = 0.0011215667, step = 37601 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.306\n",
      "INFO:tensorflow:loss = 0.0020716758, step = 37701 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.193\n",
      "INFO:tensorflow:loss = 0.010893825, step = 37801 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.403\n",
      "INFO:tensorflow:loss = 0.019541685, step = 37901 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.954\n",
      "INFO:tensorflow:loss = 0.002741877, step = 38001 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 452.421\n",
      "INFO:tensorflow:loss = 0.030116906, step = 38101 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.558\n",
      "INFO:tensorflow:loss = 0.01258922, step = 38201 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.831\n",
      "INFO:tensorflow:loss = 0.04787904, step = 38301 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.812\n",
      "INFO:tensorflow:loss = 0.011021154, step = 38401 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.256\n",
      "INFO:tensorflow:loss = 0.05263021, step = 38501 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.308\n",
      "INFO:tensorflow:loss = 0.0067792824, step = 38601 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.959\n",
      "INFO:tensorflow:loss = 0.0153978225, step = 38701 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.934\n",
      "INFO:tensorflow:loss = 0.008099333, step = 38801 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.391\n",
      "INFO:tensorflow:loss = 0.01013566, step = 38901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.956\n",
      "INFO:tensorflow:loss = 0.028535584, step = 39001 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.512\n",
      "INFO:tensorflow:loss = 0.049249277, step = 39101 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.498\n",
      "INFO:tensorflow:loss = 0.042704288, step = 39201 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.35\n",
      "INFO:tensorflow:loss = 0.038782775, step = 39301 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.817\n",
      "INFO:tensorflow:loss = 0.03796391, step = 39401 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.533\n",
      "INFO:tensorflow:loss = 0.008710507, step = 39501 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 456.088\n",
      "INFO:tensorflow:loss = 0.0066964, step = 39601 (0.219 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 477.569\n",
      "INFO:tensorflow:loss = 0.015949624, step = 39701 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.18\n",
      "INFO:tensorflow:loss = 0.007895786, step = 39801 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.033\n",
      "INFO:tensorflow:loss = 0.0019254872, step = 39901 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 451.337\n",
      "INFO:tensorflow:loss = 0.011241112, step = 40001 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.533\n",
      "INFO:tensorflow:loss = 0.020417571, step = 40101 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.003\n",
      "INFO:tensorflow:loss = 0.020271366, step = 40201 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 453.116\n",
      "INFO:tensorflow:loss = 0.02745831, step = 40301 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.198\n",
      "INFO:tensorflow:loss = 0.0059565185, step = 40401 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.849\n",
      "INFO:tensorflow:loss = 0.0191252, step = 40501 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.712\n",
      "INFO:tensorflow:loss = 0.008302279, step = 40601 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.186\n",
      "INFO:tensorflow:loss = 0.055987064, step = 40701 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 332.581\n",
      "INFO:tensorflow:loss = 0.020462863, step = 40801 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.286\n",
      "INFO:tensorflow:loss = 0.0026853946, step = 40901 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.963\n",
      "INFO:tensorflow:loss = 0.0052841185, step = 41001 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.071\n",
      "INFO:tensorflow:loss = 0.011943315, step = 41101 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.581\n",
      "INFO:tensorflow:loss = 0.040056385, step = 41201 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.848\n",
      "INFO:tensorflow:loss = 0.013393459, step = 41301 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.478\n",
      "INFO:tensorflow:loss = 0.0110363625, step = 41401 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.101\n",
      "INFO:tensorflow:loss = 0.0029476883, step = 41501 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.745\n",
      "INFO:tensorflow:loss = 0.02055639, step = 41601 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.652\n",
      "INFO:tensorflow:loss = 0.008450204, step = 41701 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.102\n",
      "INFO:tensorflow:loss = 0.019041909, step = 41801 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.532\n",
      "INFO:tensorflow:loss = 0.011713472, step = 41901 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.088\n",
      "INFO:tensorflow:loss = 0.0046723215, step = 42001 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.029\n",
      "INFO:tensorflow:loss = 0.010695976, step = 42101 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.046\n",
      "INFO:tensorflow:loss = 0.03315858, step = 42201 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.353\n",
      "INFO:tensorflow:loss = 0.0045664143, step = 42301 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.351\n",
      "INFO:tensorflow:loss = 0.009645647, step = 42401 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 313.196\n",
      "INFO:tensorflow:loss = 0.014032876, step = 42501 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.336\n",
      "INFO:tensorflow:loss = 0.0035731632, step = 42601 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.438\n",
      "INFO:tensorflow:loss = 0.013935933, step = 42701 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.478\n",
      "INFO:tensorflow:loss = 0.017933432, step = 42801 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.134\n",
      "INFO:tensorflow:loss = 0.03173732, step = 42901 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.345\n",
      "INFO:tensorflow:loss = 0.009820278, step = 43001 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.219\n",
      "INFO:tensorflow:loss = 0.017674752, step = 43101 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.864\n",
      "INFO:tensorflow:loss = 0.018935682, step = 43201 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.684\n",
      "INFO:tensorflow:loss = 0.0027709226, step = 43301 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.848\n",
      "INFO:tensorflow:loss = 0.015944123, step = 43401 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.555\n",
      "INFO:tensorflow:loss = 0.011771572, step = 43501 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.368\n",
      "INFO:tensorflow:loss = 0.012271821, step = 43601 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.354\n",
      "INFO:tensorflow:loss = 0.012420468, step = 43701 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.152\n",
      "INFO:tensorflow:loss = 0.008495367, step = 43801 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.461\n",
      "INFO:tensorflow:loss = 0.018143428, step = 43901 (0.229 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 44000 into /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03785909.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x119f41b38>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [tf.feature_column.numeric_column('X', shape=[28 * 28])]\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_train}, y=y_train, num_epochs=40, batch_size=50,\n",
    "                                              shuffle=True)\n",
    "dnn_clf.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first creates a set of real valued columns from the training set. Then we create the `DNNClassifier` and we wrap it in a sklearn compatibility helper. Finally, we run 40,000 training iterations using batches of 50 instances. If you run this code on the dataset after scaling it, you'll get a model that achieves around 98.2% accuracy on the test set!\n",
    "\n",
    "Under the hood, the `DNNClassifier` class creates all the neuron layers based on the ReLU activation function (we can change this by setting the `activation_fn` hyperparameter). The output layer relies on softmax, and the cost function is cross entropy (from chapter 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-19-05:39:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck/model.ckpt-44000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-19-05:39:35\n",
      "INFO:tensorflow:Saving dict for global step 44000: accuracy = 0.9804, average_loss = 0.107365295, global_step = 44000, loss = 13.590544\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 44000: /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck/model.ckpt-44000\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9804,\n",
       " 'average_loss': 0.107365295,\n",
       " 'loss': 13.590544,\n",
       " 'global_step': 44000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/pz/0k_47k855d194vh0354xcvrc0000gn/T/tmplpcyceck/model.ckpt-44000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "y_pred_iter = dnn_clf.predict(input_fn=test_input_fn)\n",
    "y_pred = list(y_pred_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use Keras for this task. The code would look like the following:\n",
    "\n",
    "```\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(300, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN Using Plain TensorFlow\n",
    "\n",
    "If you want more control over the architecture of the network, you may prefer to use TensorFlow's lower-level Python API. We'll build the same model as above using this API implementing mini-batch gradient descent to train it on the MNIST dataset.\n",
    "\n",
    "Step 1 is building the graph in the construction phase, and step two is the execution phase where we run the graph to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Phase\n",
    "\n",
    "First, we need to specify the inputs and outputs and set the number of hidden neurons in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder nodes will represent the training data and the targets. We're only partially defining the shape of __X__ cause we know it'll be a 2D matrix with instances along the first dimension and features along the second, and we know we'll have one feature per pixel for 784 features, but we don't know how many instances the training batches will contain. Hence the shape must be `(None, n_inputs)`. Similarly, we know that `y` will be a 1D tensor with one entry per instance, but we don't know sizes so the shape must be `(None)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the ANN. `X` will act as the input layer; during the execution phase, it will be replaced with one training batch at a time *note that ll instance in a training batch will be processed simultaneously by the neural net).* Now you need to create the two hidden layers and the output layer. The two hidden layers will really only differ by the inputs they're connected to and the number of neurons, and the output layer will be softmax instead of ReLU. Time to write a function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        std_dev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=std_dev)\n",
    "        W = tf.Variable(init, name='kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        \n",
    "        return activation(Z) if activation is not None else Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the code down line by line:\n",
    "\n",
    "1) Create a namescope using the name of the layer; it will contain all of the computation nodes for this neuron layer. This is optional, but the graph will look much nicer in TensorBoard if the nodes are well organized.\n",
    "\n",
    "2) Get the number of inputs by looking up the input matrix's shape and getting the size of the second dimension (first is for instances)\n",
    "\n",
    "3) Next three lines create a `W` variable that will hold the weights matrix (often called the *kernel* of the layer). It will be a 2D matrix containing all the connection weights between each input and each neuron; hense, its shape will be (n_inputs, n_neurons). It'll be randomly initialized using a truncated Gaussian distribution with standard deviation of $\\frac{2}{\\sqrt{n_{\\text{inputs}} + n_{\\text{neurons}}}}$ Using this specific standard deviation helps the algo converge must faster (more to come in chapter 11). It's important to initialize connection weights randomly for all hidden layers to avoid any symmetries that Gradient Descent wouldn't be able to break.\n",
    "\n",
    "4) The nxt line creates a `b` variable for biases, initialized to 0 (no syhmmetry issue in this case) with one bias param per neuron.\n",
    "\n",
    "5) WE create a subgraph to compute $\\textbf{Z} = \\textbf{X} \\cdot \\textbf{W} + \\textbf{b}$. This vectorized implementation will efficiently compute the weighted sums of the inputs plus the bias term for each and every neuron in the layer, for all instanes in the batch in just one shot. *Note: adding a 1D array to a 2D matrix with the same number of columns results in the 1D array being added to every row in the matrix. This is known as *broadcasting*\n",
    "\n",
    "6) Finally, if an `activation` param is provided such as `tf.nn.relu`, then the code returns `activation(Z)` otherwise it returns just `Z`.\n",
    "\n",
    "Okay so we can create a layer, so let's make a network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name='outputs')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that `logits` is the output __before__ going through softmax activation. We'll handle softmax computation later*\n",
    "\n",
    "TensorFlow comes with functinos to create standard neural network lauyers, so there usually isn't a need to define your own `neuron_layer` function in the way that we did. The function `tf.layers.dense()` creates a fully connected layer, where all of the inputs are connected to all the neuron in the layer. It creates the weights and biases variables (named `kernel` and `bias` respectively) using the appropriate initialization strategy, and the activation function can be set via the `activation` hyperparameter.\n",
    "\n",
    "The following code will use the built-in function instead of our own custom one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model ready to go, we'll need our cost function. We'll use cross entropy (the one that penalizes models that estimate a low probability for the target class). TensorFlow provides a few functions for computing cross entropy. The one we'll use is called `sparse_soft_max_cross_entropy_with_logits()`; it computes cross entropy based on the \"logits\" (output of the network *before* going through softmax activation) and it expects labels in the form of integers ranging from 0 to the number of classes - 1 (so for us, 0-9). This will give a 1D tensor containing the cross entropy for each instance. We can then use `reduce_mean()` to compute the mean cross entropy over all instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The `sparse_soft` function is equivalent to applying the softmax activation function and then computing the cross entropy, but it's more efficent and it properly takes care of corner cases. When logits are large, floating-point rounding errors may cause the output to be exactly equal to 0 or 1, and in this case the cross entropy equation would contain a $log(0)$ term, equal to negative infinity. The function solves this by computing $log(\\epsilon)$ instead, where $\\epsilon$ is a tiny positive number. This is why we didn't apply the softmax function earlier. There's also another function called `softmax_cross_entropy_with_logits()` that takes labels in the form of one-hot vectors instead of ints from 0 to the number of classes minus 1.*\n",
    "\n",
    "We have the neural net, we have the cost function, and now we need to define a `GradientDescentOptimizer` that'll tweak model params to minimize cost function. This is the same thing we did last chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last important step in construction is to specify how to evaluate the model. We'll simply use accuracy as our metric. First, for each instance, determine if the neural net's prediction is correct by checking whether or not the highest logit corresponds to the target class. For this, you can use the `in_top_k()` function. This returns a 1D tensor full of boolean values, so we'll need to cast these to floats and then compute the average to get the overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll create an initializer node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction is complete! This was less than 40 lines, but we did a lot: we created placeholders for inputs and targets, created a function to build the neuron layer, used that function to create a DNN, defined the cost function, created an optimizer, and defined the performance metric. Now to execution!\n",
    "\n",
    "### Execution Phase\n",
    "\n",
    "This part is shorter and simpler. First up, load the MNIST data. We'll use sklearn for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.9 Validation accuracy: 0.9024\n",
      "1 Batch accuracy: 0.92 Validation accuracy: 0.9254\n",
      "2 Batch accuracy: 0.94 Validation accuracy: 0.9372\n",
      "3 Batch accuracy: 0.9 Validation accuracy: 0.9416\n",
      "4 Batch accuracy: 0.94 Validation accuracy: 0.9472\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9512\n",
      "6 Batch accuracy: 1.0 Validation accuracy: 0.9548\n",
      "7 Batch accuracy: 0.94 Validation accuracy: 0.961\n",
      "8 Batch accuracy: 0.96 Validation accuracy: 0.9622\n",
      "9 Batch accuracy: 0.94 Validation accuracy: 0.9648\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9656\n",
      "11 Batch accuracy: 0.98 Validation accuracy: 0.9666\n",
      "12 Batch accuracy: 0.98 Validation accuracy: 0.9684\n",
      "13 Batch accuracy: 0.98 Validation accuracy: 0.9704\n",
      "14 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.9718\n",
      "16 Batch accuracy: 0.98 Validation accuracy: 0.9726\n",
      "17 Batch accuracy: 1.0 Validation accuracy: 0.9728\n",
      "18 Batch accuracy: 0.98 Validation accuracy: 0.9744\n",
      "19 Batch accuracy: 0.98 Validation accuracy: 0.9758\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "21 Batch accuracy: 1.0 Validation accuracy: 0.9734\n",
      "22 Batch accuracy: 0.96 Validation accuracy: 0.9752\n",
      "23 Batch accuracy: 0.98 Validation accuracy: 0.9764\n",
      "24 Batch accuracy: 0.98 Validation accuracy: 0.976\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
      "26 Batch accuracy: 0.92 Validation accuracy: 0.9768\n",
      "27 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "28 Batch accuracy: 0.94 Validation accuracy: 0.978\n",
      "29 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9776\n",
      "31 Batch accuracy: 1.0 Validation accuracy: 0.978\n",
      "32 Batch accuracy: 0.96 Validation accuracy: 0.9778\n",
      "33 Batch accuracy: 0.98 Validation accuracy: 0.9784\n",
      "34 Batch accuracy: 0.98 Validation accuracy: 0.9784\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9784\n",
      "36 Batch accuracy: 1.0 Validation accuracy: 0.9792\n",
      "37 Batch accuracy: 1.0 Validation accuracy: 0.9786\n",
      "38 Batch accuracy: 0.98 Validation accuracy: 0.9798\n",
      "39 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code opens a session and initializes all variables. It then runs the training loop and at the end of each epoch, it evaluates the model on the last mini-batch and on the validation set and reports the accuracy.\n",
    "\n",
    "### Using the Neural Network\n",
    "\n",
    "Now that the network is trained, you can use it for predictions. To do this, you'd reuse the construction phase but change the execution phase to restore from a previous session.\n",
    "\n",
    "The next section of the book talks about fine-tuning hyperparams, so read the book to learn this info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### 9) Train a deep MLP on the MNIST dataset and aim for 98% precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers setup\n",
    "n_inputs = 28 * 28 # number of pixels per MNIST photo\n",
    "n_hidden1 = 300 # number of nodes in 1st layer\n",
    "n_hidden2 = 100 # number of nodes in 2nd layer\n",
    "n_output = 10 # number of output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up placeholders\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the neural net\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_output, name='outputs')\n",
    "    \n",
    "# set up the loss function\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "# set up the training system\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# set up the evaluation system\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "# initialize variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's implement early stopping. We'll need the validation set for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was interrupted, continuing at epoch 1\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_deep_mnist_model.ckpt\n",
      "Epoch: 0 \tValidation accuracy: 92.540%\n",
      "Epoch: 5 \tValidation accuracy: 95.480%\n",
      "Epoch: 10 \tValidation accuracy: 96.660%\n",
      "Epoch: 15 \tValidation accuracy: 97.260%\n",
      "Epoch: 20 \tValidation accuracy: 97.340%\n",
      "Epoch: 25 \tValidation accuracy: 97.680%\n",
      "Epoch: 30 \tValidation accuracy: 97.800%\n",
      "Epoch: 35 \tValidation accuracy: 97.920%\n",
      "Epoch: 40 \tValidation accuracy: 98.080%\n",
      "Epoch: 45 \tValidation accuracy: 98.080%\n",
      "Epoch: 50 \tValidation accuracy: 98.000%\n",
      "Epoch: 55 \tValidation accuracy: 98.100%\n",
      "Epoch: 60 \tValidation accuracy: 98.140%\n",
      "Epoch: 65 \tValidation accuracy: 98.160%\n",
      "Epoch: 70 \tValidation accuracy: 98.200%\n",
      "Epoch: 75 \tValidation accuracy: 98.180%\n",
      "Epoch: 80 \tValidation accuracy: 98.220%\n",
      "Epoch: 85 \tValidation accuracy: 98.200%\n",
      "Epoch: 90 \tValidation accuracy: 98.200%\n",
      "Epoch: 95 \tValidation accuracy: 98.240%\n",
      "Epoch: 100 \tValidation accuracy: 98.200%\n",
      "Epoch: 105 \tValidation accuracy: 98.260%\n",
      "Epoch: 110 \tValidation accuracy: 98.280%\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Set up directories for the model\n",
    "checkpoint_path = '/tmp/my_deep_mnist_model.ckpt'\n",
    "checkpoint_epoch_path = checkpoint_path + '.epoch'\n",
    "final_model_path = './my_deep_mnist_model'\n",
    "\n",
    "# Prepare for early stopping\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the file exists, load it and load the epoch number\n",
    "        with open(checkpoint_epoch_path, 'rb') as infile:\n",
    "            start_epoch = int(infile.read())\n",
    "        print(\"Training was interrupted, continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, 'wb') as outfile:\n",
    "                outfile.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_deep_mnist_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(checkpoint_epoch_path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    \n",
    "accuracy_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
